{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8e7732",
   "metadata": {},
   "source": [
    "# Part-II: Decision Trees \n",
    "\n",
    "Objective: \n",
    "To implement Decision Tree classifiers and understand their structure, splits, \n",
    "and overfitting characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19edb6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 1. What is entropy and information gain?\n",
    "- Entropy measures the impurity or randomness in a dataset. A pure node (all samples from one class) has entropy = 0.\n",
    "\n",
    "- Information Gain is the reduction in entropy after a dataset is split on an attribute. It helps identify the best feature to split on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2b767",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 2. Difference between Gini Index and Entropy\n",
    "\n",
    "- Entropy comes from information theory. It quantifies the amount of uncertainty or disorder in a dataset. If a node contains only one class, its entropy is zero (perfectly pure).\n",
    "    - The formula involves logarithms and is more computationally intensive.\n",
    "    - Itâ€™s commonly used in the ID3 algorithm.\n",
    "    - âˆ’ âˆ‘ğ‘ğ‘–log2(ğ‘ğ‘–)\n",
    "\n",
    "- Gini Index, on the other hand, is a simpler metric that measures the probability of misclassifying a randomly chosen element. \n",
    "    - It doesnâ€™t use logarithms, making it faster to compute. \n",
    "    - Itâ€™s typically used in the CART algorithm.\n",
    "    - 1 âˆ’ âˆ‘ğ‘ğ‘–^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a5077",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 3. How can a decision tree overfit? How can this be avoided?\n",
    "- Overfitting occurs when the tree learns noise and details from training data, reducing generalization.\n",
    "\n",
    "- Avoidance Techniques:\n",
    "\n",
    "    - Limit tree depth (max_depth)\n",
    "\n",
    "    - Set minimum samples per split (min_samples_split)\n",
    "\n",
    "    - Use pruning (post or pre-pruning)\n",
    "\n",
    "    - Apply ensemble methods like Random Forest"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
